# Questions
1. First let’s get ready to run `x86.py` with the flag `-p flag.s`. This code “implements” locking with a single memory flag. Can you understand what the assembly code is trying to do?
    - Yes. There's a loop with a(n attempted) lock implemented with `flag`. The lock is acquired, `count` is incremented, and the procedure loops until `%bx <= 0`.

2. When you run with the defaults, does `flag.s` work as expected? Does it produce the correct result? Use the `-M` and `-R` flags to trace variables and registers (and turn on `-c` to see their values). Can you predict what value will end up in flag as the code runs?
    - Because the program only executes the loop once on each thread the result is correct. The final value in flag is 2. 

3. Change the value of the register `%bx` with the `-a` flag (e.g., `-a bx=2,bx=2` if you are running just two threads). What does the code do? How does it change your answer for the question above?
    - Again, for two loops there is no thread interleaving, so `count` will be correct at 4.

4. Set `bx` to a high value for each thread, and then use the `-i` flag to generate different interrupt frequencies; what values lead to a bad outcomes? Which lead to good outcomes?
    - With the default settings (50  instruction time slices), the 2nd thread will spin wait for it's entire 1st time slice. On the 1st thread's 2nd time slice it get's interrupted just after `mov flag %ax`, which means that when it resumes it will believe that it the lock is free. It so happens that when it does resume, thread 2 is going to be in the critical section. So any number of loops from `-a bx=10,bx=10` and up will get incorrect results. 
    - changing the interrupt to 1 or 2 get's the worst possible results, as you might expect. Effectively halving the expected result.
    - for this program, any multiple of 11 will get perfect results (the number of instructions in assembly file). Everything else will be incorrect, and will fall between 50% to 100% correct. 

5. Now let’s look at the program `test-and-set.s`. First, try to understand the code, which uses the `xchg` instruction to build a simple locking primitive. How is the lock acquire written? How about lock release?
    - This is essentially the same code as `flag.s`, except that we use an atomic instruction to load/store the lock value. Small change, big difference!

```
mov  $1, %ax        
xchg %ax, mutex     # atomic swap of 1 and mutex
test $0, %ax        # if we get 0 back: lock is free!
jne  .acquire       # if not, try again
```

6. Now run the code, changing the value of the interrupt interval (`-i`) again, and making sure to loop for a number of times. Does the code always work as expected? Does it sometimes lead to an inefficient use of the CPU? How could you quantify that?
    - The count is always correct. 
    - Again, the program is 11 instructions long. Interrupt intervals of 11 lead to no CPU waste. 
    - Since 6/11 instructions are performed under a lock. For `./x86.py -p test-and-set.s -M count,mutex -R ax -a bx=20,bx=20`:

| Interrupt interval | clock ticks |
| ------------------ | ----------- |
| 1 | 1204 |
| 2 | 1011 |
| 3 | 808 |
| 4 | 793 |
| 5 | 718 |
| 6 | 787 |
| 7 | 757 |
| 8 | 642 |
| 9 | 660 |
| 10 | 641 |
| 11 | 483 |
| 12 | 800 |
| 13 | 692 |

7. Use the `-P` flag to generate specific tests of the locking code. For example, run a schedule that grabs the lock in the first thread, but then tries to acquire it in the second. Does the right thing happen? What else should you test?
    - I expect that this code will work fine. This is a good test: `./x86.py -p test-and-set.s -M count,mutex -R ax -a bx=3,bx=3 -P 0011111 -c | less`
    - Thinking like a malicious scheduler, I'd want to test a context switch directly after `xchg` is is executed. Since it's an atomic instruction, there can't be a race condition. 

8. Now let’s look at the code in `peterson.s`, which implements Peterson’s algorithm (mentioned in a sidebar in the text). Study the code and see if you can make sense of it.
    - Yes. It's eaiser to think of `1 - self` as `other`, in my opinion. i.e. `%cx` is the tid of the other thread. 

9. Now run the code with different values of `-i`. What kinds of different behavior do you see?
    - There seems to be an error in the output of `peterson.s`, such that `%cx` is computed incorrectly, and `flag` for each thread is evaluated as being `0` on lines 30-31. As such there's a race condition when `-i` is set to 1 or 2. 

10. Can you control the scheduling (with the `-P` flag) to “prove” that the code works? What are the different cases you should show hold? Think about mutual exclusion and deadlock avoidance.
    - Again, there's a problem with the simulator. But I would be specifically testing the points at which `turn` and `flag` is changed. 

11. Now study the code for the ticket lock in `ticket.s`. Does it match the code in the chapter?
    - yes, it seems to.

12. Now run the code, with the following flags: `-a bx=1000,bx=1000` (this flag sets each thread to loop through the critical 1000 times). Watch what happens over time; do the threads spend much time spinning waiting for the lock?
    - Interesting! Since the lock is released on each loop interation each thread only has 1 turn per time slice. So progress is _slow_, and a lot of time (maybe 40%?) is spent spinning. 

13. How does the code behave as you add more threads?
    - Much the same. More threads doing very little useful work per time slice.

14. Now examine `yield.s`, in which we pretend that a yield instruction enables one thread to yield control of the CPU to another (realistically, this would be an OS primitive, but for the simplicity of simulation, we assume there is an instruction that does the task). Find a scenario where `test-and-set.s` wastes cycles spinning, but `yield.s` does not. How many instructions are saved? In what scenarios do these savings arise?
    - `./x86.py -p test-and-set.s -M count,mutex -R ax -a bx=50 -c -i 29 | wc -l` returns 1797
    - `./x86.py -p yield.s -M count,mutex -R ax -a bx=50 -c -i 29 | wc -l` returns 1327
    - So over 50 iterations, under those conditions, we're looking at 470 less clock cycles to complete with yield. 
    - The savings arise when a context switch occurs while a thread is in a critical section. The newly running thread can't do it's job. It takes 6 instructions to work that out and yield. 

15. Finally, examine `test-and-test-and-set.s`. What does this lock do? What kind of savings does it introduce as compared to `test-and-set.s`?
    - It tests to see if `mutex == 0` before attempting the `xchg` instruction. This behaves more like a compare-and-swap. The win here is that we're only writing to `mutex` when there's a fair chance we'll get the lock. That means less bus traffic and better cache coherance.
